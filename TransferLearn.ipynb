{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransferLearn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akamojo/QRS-Detector/blob/master/TransferLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt9VlR3x5Lvr",
        "colab_type": "text"
      },
      "source": [
        "# Second assignment part 1\n",
        "\n",
        "**Student**: António Coelho - MAEBD - 54575 <br>\n",
        "**Student**: Urszula Barbara Walińska - MIEI - 56556 <br>\n",
        "**Teacher**: Rui Rodrigues <br>\n",
        "**Course**: Aprendizagem com dados não estruturados <br>\n",
        "**Date of delivery**: 07/06/2019\n",
        "\n",
        "The objetive of this work is to: \n",
        "  1.  Train a neuralnetwork model on the MNIST dataset (digits).\n",
        "  2.  Use the same mode to recognize handwritten letters.\n",
        "  3.  Compare the results of using transfer learning with learning all the weights from scratch on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBDH2lxM5Lvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import genfromtxt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization,Conv2D,MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense, Input\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZgsGnhZ1ZXM",
        "colab_type": "text"
      },
      "source": [
        "# 1. Training a neuralnetwork model on the MNIST dataset (digits)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaD0-hhG5Lvz",
        "colab_type": "code",
        "outputId": "c0bc8841-34d7-46c6-964b-4f5d24f53037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# load data (X sets are images and Y sets are the true classes of the examples)\n",
        "# it is already divided into train set and test set \n",
        "\n",
        "# evaluating result of learning on test set after each epoch\n",
        "# will help us to prevent from overfitting\n",
        "\n",
        "((trainX, trainY), (testX, testY)) = keras.datasets.mnist.load_data()\n",
        "\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# transforming the labels into a proper representation for learning\n",
        "trainY = keras.utils.to_categorical(trainY, 10)\n",
        "testY = keras.utils.to_categorical(testY, 10)\n",
        "\n",
        "inputs = Input(shape=(28,28,1),name='inputs')\n",
        "\n",
        "# Convolutional Layer with 32 filters, kernel of size (3,3), \n",
        "# \"same\" padding which results in the output with the same length as the original input, \n",
        "layer = Conv2D(32, (3, 3), padding=\"same\", input_shape=(28,28,1))(inputs)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "\n",
        "# Batch Normalization to force each layer to readjust mean and standard deviation after each iteration\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "layer = Conv2D(32, (3, 3), padding=\"same\")(layer)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "\n",
        "# MaxPooling of size (2,2) to aggregate the outputs in a single output for each region\n",
        "layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
        "\n",
        "# Dropout with the probability of 0.25 for regularization -> some neurons are dropped randomly\n",
        "layer = Dropout(0.25)(layer)\n",
        "layer = Conv2D(64, (3, 3), padding=\"same\")(layer)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "layer = Conv2D(64, (3, 3), padding=\"same\")(layer)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
        "layer = Dropout(0.25)(layer)\n",
        "\n",
        "features = Flatten(name='features')(layer)\n",
        "layer = Dense(512)(features)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization()(layer)\n",
        "layer = Dropout(0.5)(layer)\n",
        "layer = Dense(10)(layer)\n",
        "layer = Activation(\"softmax\")(layer)\n",
        "old_model = Model(inputs = inputs, outputs = layer)\n",
        "\n",
        "INIT_LR = 0.01\n",
        "NUM_EPOCHS = 5\n",
        "BS = 10\n",
        "\n",
        "opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
        "old_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "old_model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=BS, epochs=NUM_EPOCHS)\n",
        "old_model.save_weights('digits_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 47s 786us/sample - loss: 0.2351 - acc: 0.9333 - val_loss: 0.0418 - val_acc: 0.9863\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 42s 706us/sample - loss: 0.1234 - acc: 0.9636 - val_loss: 0.0366 - val_acc: 0.9882\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 41s 686us/sample - loss: 0.1056 - acc: 0.9678 - val_loss: 0.0337 - val_acc: 0.9883\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 43s 712us/sample - loss: 0.1030 - acc: 0.9690 - val_loss: 0.0322 - val_acc: 0.9892\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 41s 678us/sample - loss: 0.0951 - acc: 0.9719 - val_loss: 0.0315 - val_acc: 0.9890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY-GLcy81jVd",
        "colab_type": "text"
      },
      "source": [
        "In the code above we download MNIST dataset, then preprocess our data to be able to use them in the training and evaluation of results.<br><br>\n",
        "\n",
        "Then we create Neural Network model using Keras Functional API. We use several Convolutional Layers to be able to extract from the pictures of digits relevant features that will be useful later, in classification.<br>\n",
        "To prevent from overfitting we use also Dropout layers. To prevent from vanishing gradients problem we use ReLU activation function. <br>\n",
        "In the end we attach to our convolutional network the dense part, which consists of several Dense layers and BatchNormalization and Dropout.<br>\n",
        "Of course the output layer consists of 10 neurons and softmax activation function to be able to perform the classification of 10 digits.<br><br>\n",
        "\n",
        "As a loss function in our model we use categorical cross-entropy. We use also customized Stochastic Gradient Descent as optimizer (with weight decay to prevent from problems with convergence). The metric that allows us to evaluate the results of our solution is accuracy.<br>\n",
        "After 5 epochs it reaches 99% on validation dataset.<br><br>\n",
        "\n",
        "At the end we save the weights of our model to be able to use them in the process of Transfer Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8RxYpJu472M",
        "colab_type": "text"
      },
      "source": [
        "# 2. Using the same mode to recognize handwritten letters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZTkUACg5Bz9",
        "colab_type": "text"
      },
      "source": [
        "We can take advantage from the model trained and saved before in the similar multi classification problem. This process is called Transfer Learning. It is used when 2 different tasks share relevant factors. In this case we will use the previous model build for classifying digits to classify letters.<br><br>\n",
        "\n",
        "First, we download our data using HTTP request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhGF-OVyIvbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "\n",
        "resp = urlopen(\"https://docentes.fct.unl.pt/rapr/files/transferlearningdata.zip\")\n",
        "data = np.load(BytesIO(resp.read()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFLzncvcOgDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = data['imagesLettersTrain.npy']\n",
        "trainY = data['labelsTrain.npy']\n",
        "testX = data['imagesLettersTest.npy']\n",
        "testY = data['labelsTest.npy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91bocXNmPE3i",
        "colab_type": "code",
        "outputId": "dbbbdb4e-37a4-48b7-f5c5-fee60ba552f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(testX[90].reshape(28, 28),cmap='Greys')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ffa5180fe80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEIdJREFUeJzt3WuMVVWaxvHnFcsgqNyqJIiXcrwR\nb1OYEzOmS2XidIMKan8x+qGDCXZ1QhPH0CZjmA9jTDRmMt1qyKQTHIk4ae1GW/GOrURFg1EPiCCt\nDmCQhkBRgNq0KFLwzofamFJrr3U8t32q1/+XEE6d96zaL7vq4VzW3nuZuwtAeo4qugEAxSD8QKII\nP5Aowg8kivADiSL8QKIIP5Aowg8kivADiTq6mRtrb2/3zs7OZm4SSMqWLVu0e/duq+SxNYXfzGZI\nul/SCEn/4+73hB7f2dmpcrlcyyYBBJRKpYofW/XLfjMbIem/JV0p6VxJN5rZudV+PwDNVct7/osl\nbXL3j939a0m/l3RtfdoC0Gi1hH+ypL8M+npbdt+3mFmPmZXNrNzX11fD5gDUU8M/7Xf3Re5ecvdS\nR0dHozcHoEK1hH+7pFMGfX1ydh+AYaCW8L8j6SwzO93MjpF0g6Sn69MWgEareqrP3fvNbJ6kFzUw\n1bfY3TfUrTMADVXTPL+7Py/p+Tr1AqCJOLwXSBThBxJF+IFEEX4gUYQfSBThBxJF+IFEEX4gUYQf\nSBThBxJF+IFEEX4gUYQfSFRTL93dSO4erB84cCBYb2trC9ZHjBjxg3tCbWI/U7OKrlCNHDzzA4ki\n/ECiCD+QKMIPJIrwA4ki/ECiCD+QqGE1zx+a9/3oo4+CYx966KFg/eqrrw7Wu7u7c2vMN1env78/\nWN+8eXOwPmbMmGC9vb09t3b00cPqV78heOYHEkX4gUQRfiBRhB9IFOEHEkX4gUQRfiBRNU12mtkW\nSfskHZLU7+6lejSVJzTPv3LlyuDYhQsXBuvPPfdcsP7WW2/l1kaNGhUci6GtWrUqWJ8/f36wPnr0\n6GD93nvvza1NnTo1ODaFYzfqcaTDP7v77jp8HwBNxMt+IFG1ht8l/cnMVptZTz0aAtActb7s73b3\n7WZ2oqSXzOxDd//Wm+/sP4UeSTr11FNr3ByAeqnpmd/dt2d/75L0pKSLh3jMIncvuXupo6Ojls0B\nqKOqw29mo83s+CO3Jf1E0vv1agxAY9Xysn+ipCezKZGjJT3i7svr0hWAhqs6/O7+saR/rGMvNTl0\n6FCwfvDgwWA9dj2APXv25NaY58/39ddf59YWLFgQHPvuu+8G67Fz8svlcm6tq6srODaFeX6m+oBE\nEX4gUYQfSBThBxJF+IFEEX4gUcPq+sVfffVVbi02LXT48OGa6suX5x/CMGfOnODYo45q3f9jY//u\n2BTq1q1bg/Vbb701t/bmm28Gx8aW6A5dmluSJkyYEKynrnV/KwE0FOEHEkX4gUQRfiBRhB9IFOEH\nEkX4gUQNq3n+/fv359befvvt4NjYfHbsFM4zzjgjWC9S6N/2+eefB8e++OKLwfonn3wSrC9btixY\nX7NmTW4tNo8fO2V37ty5wfqMGTNya6187EWzsAeARBF+IFGEH0gU4QcSRfiBRBF+IFGEH0hUS83z\nx+Z9e3t7c2ufffZZQ7e9c+fOmr5/SOwYhNB1DCRp3bp1ubUHHnggOPaxxx6radv9/f3Bekjs2Ipz\nzjknWL/yyiuDdS6pHsYzP5Aowg8kivADiSL8QKIIP5Aowg8kivADiYrO85vZYkkzJe1y9/Oz+8ZL\n+oOkTklbJF3v7p/W2kxs3nfMmDG5tWOPPbbWzQft3bs3txa6zoAkffppeNe8/vrrwfqrr74arD/7\n7LO5tb6+vuDY2HX5axX6mV5yySXBsXfffXewfuGFF1a9bVT2zP+QpO9eFeF2SSvc/SxJK7KvAQwj\n0fC7+0pJ333au1bSkuz2EknX1bkvAA1W7Xv+ie6+I7u9U9LEOvUDoElq/sDPBw6Kzz0w3sx6zKxs\nZuXY+08AzVNt+HvNbJIkZX/vynuguy9y95K7lzo6OqrcHIB6qzb8T0uand2eLemp+rQDoFmi4Tez\nRyW9KekcM9tmZnMk3SPpx2a2UdK/ZF8DGEai8/zufmNO6Yo69xIVmstva2tr6LZXrFiRW1u7dm1w\n7KpVq4L12Br3sXPqY9cDqEXs+vYjR44M1ru6unJrS5cuDY498cQTg/XYdf0RxhF+QKIIP5Aowg8k\nivADiSL8QKIIP5Ao5koq9Mwzz1Q9tpFTcTGxU51PO+20YL27uztYnzZtWtX1SZMmBcdySm5j8cwP\nJIrwA4ki/ECiCD+QKMIPJIrwA4ki/ECimOfPxJboruUS17H56thps7H6mWeemVubN29ecOysWbOC\n9QkTJgTrsVN6Y72jOPxkgEQRfiBRhB9IFOEHEkX4gUQRfiBRhB9IFPP8TTBixIhgfe7cucH6Lbfc\nEqyHzouPnc/POfPp4pkfSBThBxJF+IFEEX4gUYQfSBThBxJF+IFERef5zWyxpJmSdrn7+dl9d0j6\nuaS+7GEL3P35RjV5RGi+vL29vdGbr1rsWgCPP/54sD5lypRg/eabb86tMY+PPJU88z8kacYQ99/r\n7l3Zn4YHH0B9RcPv7isl7W1CLwCaqJb3/PPMbJ2ZLTazcXXrCEBTVBv+30o6Q1KXpB2Sfp33QDPr\nMbOymZX7+vryHgagyaoKv7v3uvshdz8s6QFJFwceu8jdS+5e6ujoqLZPAHVWVfjNbPBpZD+V9H59\n2gHQLJVM9T0qaZqkdjPbJuk/JE0zsy5JLmmLpF80sEcADRANv7vfOMTdDzagl6jRo0fn1qZPnx4c\n+8YbbwTrBw8erKqnetixY0ewfuedd1b9vWfOnBmsx67Lz/UA/n5xhB+QKMIPJIrwA4ki/ECiCD+Q\nKMIPJGpYXbo7dErvCSecUNP3jk1ZxZbwrmVsbNu9vb3B+m233ZZbW7hwYXDs5ZdfHqzHLht+9tln\nB+uhfxvLdxeLvQ8kivADiSL8QKIIP5Aowg8kivADiSL8QKKG1Tx/yNixY4P1Y445Jljv7+8P1hs5\nX3348OGaxn/55Ze5tQ8//DA4dtOmTcH6nj17gvXYcQLjx4/PrV1zzTXBsbHTiTlOoDbsPSBRhB9I\nFOEHEkX4gUQRfiBRhB9IFOEHEjWs5vlD87rd3d3BsZMnTw7WN27cWPW2Z82aFRwbuzz26tWrg/X9\n+/cH65s3b86txa4lEDu+4YknngjWly1bFqyHjq945ZVXgmOnTZsWrF966aXB+vHHH59bC10GXgpf\nO0KKX4NhOFzSnGd+IFGEH0gU4QcSRfiBRBF+IFGEH0gU4QcSFZ3nN7NTJD0saaIkl7TI3e83s/GS\n/iCpU9IWSde7+6eNazVs1KhRNdVjQvP8V1xxRXDsTTfdFKwfOHAgWP/iiy+C9fnz5wfrIS+88EKw\nHrpWgBS/FkHoOILFixcHxz7yyCPBeuzYjZNOOim3NmPGjODYcePGBetdXV3B+nnnnResh34fm3WM\nQCXP/P2SfuXu50r6J0m/NLNzJd0uaYW7nyVpRfY1gGEiGn533+Hua7Lb+yR9IGmypGslLcketkTS\ndY1qEkD9/aD3/GbWKWmqpLckTXT3HVlppwbeFgAYJioOv5kdJ+mPkm51978OrvnAAeRDHkRuZj1m\nVjazcl9fX03NAqifisJvZm0aCP7v3P3ImR69ZjYpq0+StGuose6+yN1L7l7q6OioR88A6iAafhv4\n6PFBSR+4+28GlZ6WNDu7PVvSU/VvD0CjVHJK748k/UzSejNbm923QNI9kpaa2RxJn0i6vjEtVia2\nRPcNN9wQrG/YsKHqbcdO/4xNMx533HHBeuyU4KVLl+bWDh06FBy7fv36YP3ll18O1pcvXx6s7969\nO7cWexu4a9eQLya/ETsNO1R/7bXXgmNjYj/zKVOmBOtLlizJrU2dOjU4tl5TgdHwu/sbkvK2Fp7g\nBtCyOMIPSBThBxJF+IFEEX4gUYQfSBThBxI1rC7dHRKbd42dohlbwvv000/PrU2cWOxpDaHTjWPL\nWMfmlC+44IJgvaenJ1gPHWfw3nvvBcfeddddwXrsOIHQqdLbtm0Ljo2JLR++b9++YH3r1q25tdjp\nwvWa5+eZH0gU4QcSRfiBRBF+IFGEH0gU4QcSRfiBRP3dzPPH5j5jyz3fd999wfr06dNza+3t7cGx\nsbn2IsX2W1tbW7A+duzYqrd92WWXBesXXXRRsB67VkHokueh8+klqbOzM1g/+eSTg/XYNRpCl/Zu\n1u9L6/5WAmgowg8kivADiSL8QKIIP5Aowg8kivADibKBlbaao1Qqeblcbtr2Bov9O2P1Vp6rx9BC\nP9PYsuix6zvExI6faNQy3KVSSeVyuaJvzm80kCjCDySK8AOJIvxAogg/kCjCDySK8AOJip7Pb2an\nSHpY0kRJLmmRu99vZndI+rmkIxdPX+Duzzeq0VoVNe+K4oR+piNHjmxiJ62pkot59Ev6lbuvMbPj\nJa02s5ey2r3u/l+Naw9Ao0TD7+47JO3Ibu8zsw8kTW50YwAa6we95zezTklTJb2V3TXPzNaZ2WIz\nG3I9LDPrMbOymZVjyysBaJ6Kw29mx0n6o6Rb3f2vkn4r6QxJXRp4ZfDroca5+yJ3L7l7qaOjow4t\nA6iHisJvZm0aCP7v3P0JSXL3Xnc/5O6HJT0g6eLGtQmg3qLht4GPTB+U9IG7/2bQ/ZMGPeynkt6v\nf3sAGqWST/t/JOlnktab2drsvgWSbjSzLg1M/22R9IuGdAigISr5tP8NSUNNmLbsnD6AOI7wAxJF\n+IFEEX4gUYQfSBThBxJF+IFEEX4gUYQfSBThBxJF+IFEEX4gUYQfSBThBxJF+IFENXWJbjPrk/TJ\noLvaJe1uWgM/TKv21qp9SfRWrXr2dpq7V3S9vKaG/3sbNyu7e6mwBgJatbdW7Uuit2oV1Rsv+4FE\nEX4gUUWHf1HB2w9p1d5atS+J3qpVSG+FvucHUJyin/kBFKSQ8JvZDDP7yMw2mdntRfSQx8y2mNl6\nM1trZuWCe1lsZrvM7P1B9403s5fMbGP295DLpBXU2x1mtj3bd2vN7KqCejvFzF4xsz+b2QYz+9fs\n/kL3XaCvQvZb01/2m9kISf8n6ceStkl6R9KN7v7npjaSw8y2SCq5e+FzwmZ2maS/SXrY3c/P7vtP\nSXvd/Z7sP85x7v5vLdLbHZL+VvTKzdmCMpMGrywt6TpJN6nAfRfo63oVsN+KeOa/WNImd//Y3b+W\n9HtJ1xbQR8tz95WS9n7n7mslLcluL9HAL0/T5fTWEtx9h7uvyW7vk3RkZelC912gr0IUEf7Jkv4y\n6Ottaq0lv13Sn8xstZn1FN3MECZmy6ZL0k5JE4tsZgjRlZub6TsrS7fMvqtmxet64wO/7+t294sk\nXSnpl9nL25bkA+/ZWmm6pqKVm5tliJWlv1Hkvqt2xet6KyL82yWdMujrk7P7WoK7b8/+3iXpSbXe\n6sO9RxZJzf7eVXA/32illZuHWllaLbDvWmnF6yLC/46ks8zsdDM7RtINkp4uoI/vMbPR2QcxMrPR\nkn6i1lt9+GlJs7PbsyU9VWAv39IqKzfnrSytgvddy6147e5N/yPpKg184r9Z0r8X0UNOX/8g6b3s\nz4aie5P0qAZeBh7UwGcjcyRNkLRC0kZJL0sa30K9/a+k9ZLWaSBokwrqrVsDL+nXSVqb/bmq6H0X\n6KuQ/cYRfkCi+MAPSBThBxJF+IFEEX4gUYQfSBThBxJF+IFEEX4gUf8P+WASHbVB04oAAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b63Oe0B45LFq",
        "colab_type": "text"
      },
      "source": [
        "Exemplary letter that we are going to classify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7NSbTICRR73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data preprocessing as before\n",
        "\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "trainY = keras.utils.to_categorical(trainY.astype(\"int\"), 26)\n",
        "testY = keras.utils.to_categorical(testY.astype(\"int\"), 26)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8cqINCR5Lv6",
        "colab_type": "code",
        "outputId": "8e3768eb-4c89-4d81-8bd6-06ac17febe4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1736
        }
      },
      "source": [
        "old_model.compile(optimizer=SGD(), loss='mse')\n",
        "\n",
        "# using weights from previous task\n",
        "old_model.load_weights('digits_model.h5')\n",
        "\n",
        "# prevent the layers from previous model from training\n",
        "for layer in old_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# attaching new dense network to the previous convolution network\n",
        "layer = Dense(512)(old_model.get_layer('features').output)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization()(layer)\n",
        "layer = Dropout(0.5)(layer)\n",
        "layer = Dense(26)(layer)\n",
        "layer = Activation(\"softmax\")(layer)\n",
        "model = Model(inputs = old_model.get_layer('inputs').output, outputs = layer)\n",
        "\n",
        "INIT_LR = 0.01\n",
        "NUM_EPOCHS = 20\n",
        "BS = 50\n",
        "\n",
        "opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# using the obtained model to fit into new data\n",
        "H = model.fit(trainX, trainY, batch_size=BS, epochs=NUM_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_1 (Ba (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_2 (Ba (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_3 (Ba (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "features (Flatten)           (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_5 (Ba (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 26)                13338     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 26)                0         \n",
            "=================================================================\n",
            "Total params: 1,687,290\n",
            "Trainable params: 1,620,506\n",
            "Non-trainable params: 66,784\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "520/520 [==============================] - 0s 622us/sample - loss: 3.4037 - acc: 0.2462\n",
            "Epoch 2/20\n",
            "520/520 [==============================] - 0s 105us/sample - loss: 0.9883 - acc: 0.7154\n",
            "Epoch 3/20\n",
            "520/520 [==============================] - 0s 107us/sample - loss: 0.6392 - acc: 0.7808\n",
            "Epoch 4/20\n",
            "520/520 [==============================] - 0s 107us/sample - loss: 0.4787 - acc: 0.8308\n",
            "Epoch 5/20\n",
            "520/520 [==============================] - 0s 101us/sample - loss: 0.3726 - acc: 0.8808\n",
            "Epoch 6/20\n",
            "520/520 [==============================] - 0s 100us/sample - loss: 0.2789 - acc: 0.9096\n",
            "Epoch 7/20\n",
            "520/520 [==============================] - 0s 98us/sample - loss: 0.2797 - acc: 0.8942\n",
            "Epoch 8/20\n",
            "520/520 [==============================] - 0s 98us/sample - loss: 0.2516 - acc: 0.9250\n",
            "Epoch 9/20\n",
            "520/520 [==============================] - 0s 99us/sample - loss: 0.2336 - acc: 0.9173\n",
            "Epoch 10/20\n",
            "520/520 [==============================] - 0s 118us/sample - loss: 0.1454 - acc: 0.9538\n",
            "Epoch 11/20\n",
            "520/520 [==============================] - 0s 104us/sample - loss: 0.1362 - acc: 0.9577\n",
            "Epoch 12/20\n",
            "520/520 [==============================] - 0s 97us/sample - loss: 0.1377 - acc: 0.9596\n",
            "Epoch 13/20\n",
            "520/520 [==============================] - 0s 97us/sample - loss: 0.1405 - acc: 0.9615\n",
            "Epoch 14/20\n",
            "520/520 [==============================] - 0s 98us/sample - loss: 0.1126 - acc: 0.9712\n",
            "Epoch 15/20\n",
            "520/520 [==============================] - 0s 95us/sample - loss: 0.1114 - acc: 0.9731\n",
            "Epoch 16/20\n",
            "520/520 [==============================] - 0s 103us/sample - loss: 0.1221 - acc: 0.9635\n",
            "Epoch 17/20\n",
            "520/520 [==============================] - 0s 96us/sample - loss: 0.1170 - acc: 0.9692\n",
            "Epoch 18/20\n",
            "520/520 [==============================] - 0s 99us/sample - loss: 0.0971 - acc: 0.9673\n",
            "Epoch 19/20\n",
            "520/520 [==============================] - 0s 100us/sample - loss: 0.0735 - acc: 0.9788\n",
            "Epoch 20/20\n",
            "520/520 [==============================] - 0s 97us/sample - loss: 0.0894 - acc: 0.9731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D7ELqLV5LwA",
        "colab_type": "code",
        "outputId": "d6e0a929-6a3d-485c-830e-e2c37c5fca98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "scores = model.evaluate(testX, testY, batch_size=BS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103080/103080 [==============================] - 5s 49us/sample - loss: 0.9138 - acc: 0.7673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMq_CVuh7Q7K",
        "colab_type": "text"
      },
      "source": [
        "As we can see, we managed to reduce slightly the number of trainable parameters by using model built for the previous problem. Moreover we managed to achieve the accuracy on test set equal 77%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0zDRhFn7jw4",
        "colab_type": "text"
      },
      "source": [
        "# 3. Comparison of the results using transfer learning with learning all the weights from scratch on the training dataset (letters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCMS3jPaGCJ",
        "colab_type": "code",
        "outputId": "6c0ca170-a304-4a56-e813-900daad417a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "inputs = Input(shape=(28,28,1),name='inputs')\n",
        "layer = Conv2D(32, (3, 3), padding=\"same\", input_shape=(28,28,1))(inputs)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "layer = Conv2D(32, (3, 3), padding=\"same\")(layer)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
        "layer = Dropout(0.25)(layer)\n",
        "layer = Conv2D(64, (3, 3), padding=\"same\")(layer)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "layer = Conv2D(64, (3, 3), padding=\"same\")(layer)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization(axis=-1)(layer)\n",
        "layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
        "layer = Dropout(0.25)(layer)\n",
        "\n",
        "features = Flatten(name='features')(layer)\n",
        "layer = Dense(512)(features)\n",
        "layer = Activation(\"relu\")(layer)\n",
        "layer = BatchNormalization()(layer)\n",
        "layer = Dropout(0.5)(layer)\n",
        "layer = Dense(26)(layer)\n",
        "layer = Activation(\"softmax\")(layer)\n",
        "new_model = Model(inputs = inputs, outputs = layer)\n",
        "\n",
        "INIT_LR = 0.01\n",
        "NUM_EPOCHS = 20\n",
        "BS = 10\n",
        "\n",
        "opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
        "new_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "new_model.fit(trainX, trainY, batch_size=BS, epochs=NUM_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "520/520 [==============================] - 1s 2ms/sample - loss: 3.3122 - acc: 0.2538\n",
            "Epoch 2/20\n",
            "520/520 [==============================] - 0s 667us/sample - loss: 2.2457 - acc: 0.4519\n",
            "Epoch 3/20\n",
            "520/520 [==============================] - 0s 665us/sample - loss: 1.8519 - acc: 0.5192\n",
            "Epoch 4/20\n",
            "520/520 [==============================] - 0s 680us/sample - loss: 1.4620 - acc: 0.5885\n",
            "Epoch 5/20\n",
            "520/520 [==============================] - 0s 665us/sample - loss: 1.2929 - acc: 0.6365\n",
            "Epoch 6/20\n",
            "520/520 [==============================] - 0s 669us/sample - loss: 1.2164 - acc: 0.6788\n",
            "Epoch 7/20\n",
            "520/520 [==============================] - 0s 676us/sample - loss: 1.0616 - acc: 0.6827\n",
            "Epoch 8/20\n",
            "520/520 [==============================] - 0s 662us/sample - loss: 0.9819 - acc: 0.7269\n",
            "Epoch 9/20\n",
            "520/520 [==============================] - 0s 659us/sample - loss: 0.7665 - acc: 0.7462\n",
            "Epoch 10/20\n",
            "520/520 [==============================] - 0s 681us/sample - loss: 0.6426 - acc: 0.7981\n",
            "Epoch 11/20\n",
            "520/520 [==============================] - 0s 665us/sample - loss: 0.6445 - acc: 0.8019\n",
            "Epoch 12/20\n",
            "520/520 [==============================] - 0s 679us/sample - loss: 0.5605 - acc: 0.8365\n",
            "Epoch 13/20\n",
            "520/520 [==============================] - 0s 668us/sample - loss: 0.6008 - acc: 0.8269\n",
            "Epoch 14/20\n",
            "520/520 [==============================] - 0s 655us/sample - loss: 0.5486 - acc: 0.8269\n",
            "Epoch 15/20\n",
            "520/520 [==============================] - 0s 662us/sample - loss: 0.5342 - acc: 0.8231\n",
            "Epoch 16/20\n",
            "520/520 [==============================] - 0s 657us/sample - loss: 0.4759 - acc: 0.8442\n",
            "Epoch 17/20\n",
            "520/520 [==============================] - 0s 647us/sample - loss: 0.4874 - acc: 0.8462\n",
            "Epoch 18/20\n",
            "520/520 [==============================] - 0s 668us/sample - loss: 0.3940 - acc: 0.8577\n",
            "Epoch 19/20\n",
            "520/520 [==============================] - 0s 655us/sample - loss: 0.3672 - acc: 0.8923\n",
            "Epoch 20/20\n",
            "520/520 [==============================] - 0s 653us/sample - loss: 0.4227 - acc: 0.8788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffa151765c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeRS6H7jalh9",
        "colab_type": "code",
        "outputId": "e85d62e8-52f1-46fe-a017-459cd7a569fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "scores = new_model.evaluate(testX, testY, batch_size=BS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103080/103080 [==============================] - 23s 225us/sample - loss: 1.7395 - acc: 0.6421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEwi_S7x74Vx",
        "colab_type": "text"
      },
      "source": [
        "As we can see, taking advantage from the previously trained model (for digits classification) resulted in higher accuracy result (77%) than learning all the weights from scratch on letters training dataset (64%). It is caused by the fact that given letters training (and validation) set was very small (520 examples for 26 letters, upper and lower case). On the other hand, a large test set of letters was provided. The convolutional network from model trained by learning weights from scratch did not manage to extract relevant features from data. Using the combination of already trained convolutional network (when the problem of classification is simillar) and new dense part of the network for this purpose gives us new opportunities and enables us to achieve satisfying results."
      ]
    }
  ]
}